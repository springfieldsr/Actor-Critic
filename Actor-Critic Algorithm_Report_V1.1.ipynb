{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic Algorithm Report V1.1\n",
    "\n",
    "This report is a summary of my changes made based on the last AC algorithm code.  \n",
    "Following figure is the result from last time.  \n",
    "\n",
    "<img src=\"./lastTime.png\" />  \n",
    "\n",
    "Clearly, the fluctuation is huge. The agent learns fast, but does not converge. Naturally, one possible cause for this fluctuation is learning rate. In the previous code, the learning rates for actor and critic were 0.001 and 0.01. In this patch, I further lowered the learning rates of actor and critic to 0.0005 and 0.001 and decay both learning rates with global step, making critc slightly faster than actor but not too fast and thus losing balance. To compensate the low learning rate, I had to adjust the cost of failure from -20 to -5; otherwise the agent will spend too much time.    \n",
    "Following figure is the result of learning rate adjustment, which prints mean reward of 20 latest episodes. As it turns out, the agent is still experiencing fluctuation, but more stable than before. And obviously, as the learning rate is much smaller, the \"convergence\" is taking more time.  \n",
    "\n",
    "<img src=\"./changeonlyLR.png\" />  \n",
    "\n",
    "As the code is using value function to update critic instead of Q function, the concern that I had before was only partially correct. Previously, I thought in the process of updating critic, as the critic is using the same network to compute both current state value and the next state value, there will be a shift in value and result in fluctuation. So I did some experiment where I had two networks, as in Double DQN, to estimate values in current and next state. But this approach did not go well, still experiencing fluctuation. The following figure is the result of using DDQN idea in establishing two networks to estimate value function.   \n",
    "\n",
    "<img src=\"./twoValueNet.png\" />  \n",
    "\n",
    "The reason for this not so successful attempt might be that value function does not shift as frequently as Q function, because value function has fewer constraint, as it does not depend on action. So this approach did not have too much influence on the fluctuation.  \n",
    "\n",
    "\n",
    "To further address the fluctuation, I experimented with the reward discounting factor and cost of failure. After dozens of tests, the optimal discouting factor is 0.9 and the cost of failure is -1. Also, in order to compare with the results of vanilla PG that I have done in spring break, I changed the reward limit to be 200, which is same as vanilla PG. The following first figure is the result of the best tuning so far, and the second figure is the result of vanilla PG, performing the same task. To clarify, vanilla PG went through 5000 episodes, and result was the mean reward of every 100 episodes, while AC went through 1400 episodes, showing mean reward of every 20 episodes, and converged significantly faster than vanilla PG.  \n",
    "\n",
    "<img src=\"./bestosfarG0.9.png\" />  \n",
    "\n",
    "<img src=\"./vanilla.png\" />  \n",
    "\n",
    "\n",
    "Summary of adjustment:   \n",
    "1. Tuned down the learning rate of actor and critic, enabling critic to learn faster than actor at a relatively slow pace  \n",
    "2. Changed the discount factor from 0.8 to 0.9, which makes the current reward more valuable.  \n",
    "3. Changed the maximum steps in each episode to be 200 to better scale the reward.  \n",
    "4. Decreased failure penalty from -20 to -1 to compensate low learning rate  \n",
    "5. Experimented with double value networks  \n",
    "  "
   ]
  }
 ]
}