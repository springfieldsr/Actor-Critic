{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   }
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor-Critic Algorithm Report\n",
    "\n",
    "This project is to solve CartPole problem from Gym OpenAI. Following is sample code of AC algorithm provided by https://github.com/MorvanZhou/Reinforcement-learning-with-tensorflow/blob/master/contents/8_Actor_Critic_Advantage/AC_CartPole.py. My understanding of each line of code is in the inline comments.\n",
    "\n",
    "To summarize, AC algorithm uses two models, a critic which represents Q function or V function, and an actor, which updates policy in the gradient ascent direction suggested by critic. The algorithm initializes policy randomly, samples tuple of (a, r, s', a') and then update policy paramters θ ← θ + αθ * Qw(s,a) * ∇θlnπθ(a|s). Meanwhile, it computes TD error of critic, and uses TD error to update parameters of critic: w ← w + αw * δt * ∇w Qw(s,a).\n",
    "\n",
    "While studying the sample code, I noticed incredibly strong fluctuation of reward, sometimes even dropping from nearly 300 to 100. I suspect the reason might be resulted from the update process of critic. When calculating TD error, it uses the same network to estimate both current state and next state Q values; the network can be destabilized by falling into feedback loops between target and estimated value. After long time tuning, the result is slightly better, but still not converging to a certain point. The resulting reward distribution is as following.\n",
    "\n",
    "<img src=\"./reward.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is the sample code with inline comments.\n",
    "\n",
    "Part 1: hyper parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reproducible\n",
    "np.random.seed(2)\n",
    "tf.set_random_seed(2)  \n",
    "\n",
    "# Hyper Parameters\n",
    "OUTPUT_GRAPH = True\n",
    "MAX_EPISODE = 1000\n",
    "DISPLAY_REWARD_THRESHOLD = 200  # renders environment if total episode reward is greater then this threshold\n",
    "MAX_EP_STEPS = 1000   # maximum time step in one episode\n",
    "RENDER = False  # rendering wastes time\n",
    "GAMMA = 0.9     # reward discount in TD error\n",
    "\n",
    "# import CartPole from gym\n",
    "env = gym.make('CartPole-v0')\n",
    "env.seed(1)  # reproducible\n",
    "env = env.unwrapped\n",
    "\n",
    "# get number of features and number of actions from environment\n",
    "N_F = env.observation_space.shape[0]\n",
    "N_A = env.action_space.n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: actor network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(object):\n",
    "    def __init__(self, sess, n_features, n_actions, lr, global_step):\n",
    "        self.sess = sess\n",
    "\n",
    "        # placeholder for states, which is an 1-D array\n",
    "        self.s = tf.placeholder(tf.float32, [1, n_features], \"state\")\n",
    "        # placeholder for action to choose later based on probability that \n",
    "        # is generated by two fully connect layers\n",
    "        self.a = tf.placeholder(tf.int32, None, \"act\")\n",
    "        # placeholder for TD error that is generated by critic\n",
    "        # this error is used to update the model of actor\n",
    "        self.td_error = tf.placeholder(tf.float32, None, \"td_error\")  # TD_error\n",
    "\n",
    "        # construct the two fully connected layers of actor\n",
    "        with tf.variable_scope('Actor'):\n",
    "            l1 = tf.layers.dense(\n",
    "                inputs=self.s,\n",
    "                units=20,    # number of hidden units\n",
    "                activation=tf.nn.relu,\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),    # weights\n",
    "                #bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='l1'\n",
    "            )\n",
    "\n",
    "            self.acts_prob = tf.layers.dense(\n",
    "                inputs=l1,\n",
    "                units=n_actions,    # output units\n",
    "                activation=tf.nn.softmax,   # get action probabilities\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                #bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='acts_prob'\n",
    "            )\n",
    "\n",
    "        # construct the loss function based on the following equation:\n",
    "        # θ ← θ + αθ * Aw(s,a) * ∇θlnπθ(a|s)\n",
    "        with tf.variable_scope('exp_v'):\n",
    "            log_prob = tf.log(self.acts_prob[0, self.a])\n",
    "            self.exp_v = tf.reduce_mean(log_prob * self.td_error)  # advantage (TD_error) guided loss\n",
    "\n",
    "        # update the network of actor\n",
    "        with tf.variable_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(lr).minimize(-self.exp_v, global_step)  # minimize(-exp_v) = maximize(exp_v)\n",
    "\n",
    "    def learn(self, s, a, td):\n",
    "        s = s[np.newaxis, :]\n",
    "        feed_dict = {self.s: s, self.a: a, self.td_error: td}\n",
    "        _, exp_v = self.sess.run([self.train_op, self.exp_v], feed_dict)\n",
    "        return exp_v\n",
    "\n",
    "    # choose action based on probability generated by current state\n",
    "    def choose_action(self, s):\n",
    "        s = s[np.newaxis, :]\n",
    "        probs = self.sess.run(self.acts_prob, {self.s: s})   # get probabilities for all actions\n",
    "        return np.random.choice(np.arange(probs.shape[1]), p=probs.ravel())   # return a int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3: critic network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(object):\n",
    "    def __init__(self, sess, n_features, lr, global_step):\n",
    "        self.sess = sess\n",
    "\n",
    "        # placeholder for states, which is an 1-D array\n",
    "        self.s = tf.placeholder(tf.float32, [1, n_features], \"state\")\n",
    "        # placeholder for value of next state\n",
    "        self.v_ = tf.placeholder(tf.float32, [1, 1], \"v_next\")\n",
    "        # placeholder for reward\n",
    "        self.r = tf.placeholder(tf.float32, None, 'r')\n",
    "\n",
    "        # construct two fully connect layers of critic\n",
    "        with tf.variable_scope('Critic'):\n",
    "            l1 = tf.layers.dense(\n",
    "                inputs=self.s,\n",
    "                units=20,  # number of hidden units\n",
    "                activation=tf.nn.relu,  # None\n",
    "                # have to be linear to make sure the convergence of actor.\n",
    "                # But linear approximator seems hardly learns the correct Q.\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                #bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='l1'\n",
    "            )\n",
    "\n",
    "            self.v = tf.layers.dense(\n",
    "                inputs=l1,\n",
    "                units=1,  # output units\n",
    "                activation=None,\n",
    "                kernel_initializer=tf.random_normal_initializer(0., .1),  # weights\n",
    "                #bias_initializer=tf.constant_initializer(0.1),  # biases\n",
    "                name='V'\n",
    "            )\n",
    "\n",
    "        # construct TD error = reward + y*value_next_state - estimated_value_current state\n",
    "        with tf.variable_scope('squared_TD_error'):\n",
    "            self.td_error = self.r + GAMMA * self.v_ - self.v\n",
    "            self.loss = tf.square(self.td_error)    # TD_error = (r+gamma*V_next) - V_eval\n",
    "        with tf.variable_scope('train'):\n",
    "            self.train_op = tf.train.AdamOptimizer(lr).minimize(self.loss)\n",
    "\n",
    "    # function to learn critic model and returns TD error\n",
    "    def learn(self, s, r, s_):\n",
    "        s, s_ = s[np.newaxis, :], s_[np.newaxis, :]\n",
    "\n",
    "        v_ = self.sess.run(self.v, {self.s: s_})\n",
    "        td_error, _ = self.sess.run([self.td_error, self.train_op],\n",
    "                                          {self.s: s, self.v_: v_, self.r: r})\n",
    "        return td_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 4: training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False)\n",
    "learning_rate_A = tf.compat.v1.train.exponential_decay(0.0005, global_step, 300, 0.98)\n",
    "learning_rate_C = tf.compat.v1.train.exponential_decay(0.001, global_step, 300, 0.98)\n",
    "\n",
    "actor = Actor(sess, n_features=N_F, n_actions=N_A, lr=learning_rate_A, global_step=global_step)\n",
    "critic = Critic(sess, n_features=N_F, lr=learning_rate_C, global_step=global_step)     # we need a good teacher, so the teacher should learn faster than the actor\n",
    "\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "if OUTPUT_GRAPH:\n",
    "    tf.summary.FileWriter(\"logs/\", sess.graph)\n",
    "\n",
    "# list of running_rewards\n",
    "re = []\n",
    "res = []\n",
    "rr = []\n",
    "\n",
    "for i_episode in range(MAX_EPISODE):\n",
    "    # get initial state\n",
    "    s = env.reset()\n",
    "    t = 0\n",
    "    sum_r = 0\n",
    "    # list of rewards in this episode\n",
    "    track_r = []\n",
    "    while True:\n",
    "        if RENDER: env.render()\n",
    "        # choose action stochastically based on current state\n",
    "        a = actor.choose_action(s)\n",
    "        # get next state, reward and end-flag taking the chosen action\n",
    "        s_, r, done, info = env.step(a)\n",
    "        # if the episode fails, reward -5\n",
    "        if done: r = -1\n",
    "        # append r to the list\n",
    "        track_r.append(r)\n",
    "        sum_r += r\n",
    "        # calculate TD error based on current state, reward and next state\n",
    "        td_error = critic.learn(s, r, s_)  # gradient = grad[r + gamma * V(s_) - V(s)]\n",
    "        actor.learn(s, a, td_error)     # true_gradient = grad[logPi(s,a) * td_error]\n",
    "\n",
    "        s = s_\n",
    "        t += 1\n",
    "\n",
    "        if done or t >= MAX_EP_STEPS:\n",
    "            ep_rs_sum = sum(track_r)\n",
    "\n",
    "            if 'running_reward' not in globals():\n",
    "                running_reward = ep_rs_sum\n",
    "            else:\n",
    "                running_reward = running_reward * 0.95 + ep_rs_sum * 0.05\n",
    "            if running_reward > DISPLAY_REWARD_THRESHOLD: RENDER = True  # rendering\n",
    "            print(\"episode:\", i_episode, \"  reward:\", int(running_reward))\n",
    "            res.append(sum_r)\n",
    "            rr.append(running_reward)\n",
    "            break\n",
    "    if i_episode % 20 == 0:\n",
    "        re.append(np.mean(res[-20:]))"
   ]
  }
 ]
}